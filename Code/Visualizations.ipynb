{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "path:str = '/Users/juanbeta/My Drive/Research/Energy/E-CVRP-TW/Code/'\n",
    "#path: str = 'C:/Users/jm.betancourt/Documents/Research/Energy//E-CVRP-TW/Code/'\n",
    "\n",
    "from E_CVRP_TW import  E_CVRP_TW, Feasibility\n",
    "env = E_CVRP_TW(path)\n",
    "\n",
    "sys.path.insert(0,path+'Experimentation/')\n",
    "import plot_performance as plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrictive criterion\n",
    "\n",
    "Testing various restriction configuration for the RCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = ['distance', 'TimeWindow','Intra-Hybrid', 'Exo-Hybrid']\n",
    "data_crit = dict()\n",
    "for instance in env.instances:\n",
    "    data_crit[instance] = dict()\n",
    "    for criterion in criterions:\n",
    "        data = plot.retrieve_const_performance(instance, path+f'Experimentation/Constructive/RCL criterion/{criterion}/')\n",
    "        data_crit[instance][criterion] = dict() \n",
    "        data_crit[instance][criterion]['min_dist'] = data[0]\n",
    "        data_crit[instance][criterion]['min_EV'] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' \\t \\t \\tDist \\t \\t \\t \\t \\tTW \\t \\t \\t \\t \\tIntra-H \\t \\t \\t \\t \\tExo-H')\n",
    "print('Instance \\tM \\tEV \\tFO \\tgap \\ts \\t \\tEV \\tFO \\tgap \\ts \\t \\tEV \\tFO \\tgap \\ts \\t \\tEV \\tFO \\tgap \\ts')\n",
    "\n",
    "objective = 'min_EV'\n",
    "\n",
    "for instance in env.sizes['l'][:10]:\n",
    "    env.load_data(instance)\n",
    "    things = [instance, len(env.Costumers)]\n",
    "    for criterion in criterions:\n",
    "        things += [len(data_crit[instance][criterion][objective]['best individual']),\n",
    "                    round(data_crit[instance][criterion][objective]['best distance'],2),\n",
    "                    data_crit[instance][criterion][objective]['gap'],\n",
    "                    round(data_crit[instance][criterion][objective]['time to find'],2),\n",
    "                    ' ']\n",
    "    print(*things,sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = env.sizes['l'][4]\n",
    "criterion = 'TimeWindow'\n",
    "objective = 'min_EV'\n",
    "\n",
    "print(f'########## Performance ##########')\n",
    "print(f'incumbent: {round(data_crit[instance][criterion][objective][\"best distance\"],2)}')\n",
    "print(f'gap: {round((data_crit[instance][criterion][objective][\"best distance\"] - env.bkFO[instance])/env.bkFO[instance]*100,2)}%')\n",
    "print(f'time to find: {round(data_crit[instance][criterion][objective][\"time to find\"],2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = env.sizes['l'][6]\n",
    "plot.plot_const_performance(data_crit[instance],instance, objective = 'min_EV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Export data to Excel\n",
    "data = data_crit \n",
    "\n",
    "d = {'instance': [], 'M':[]}\n",
    "for instance in env.instances:\n",
    "    env.load_data(instance)\n",
    "    d['instance'].append(instance)\n",
    "    d['M'].append(len(env.Costumers))\n",
    "\n",
    "for criterion in criterions:\n",
    "    d[f'{criterion}/# EV'] = [len(data[instance][criterion]['min_EV']['best individual']) for instance in env.instances]\n",
    "    d[f'{criterion}/FO'] = [round(data[instance][criterion]['min_EV']['best distance'],2) for instance in env.instances]\n",
    "    # d[f'{criterion}/gap'] = [data[instance][criterion]['gap'] for instance in env.instances]\n",
    "    d[f'{criterion}/t'] = [round(data[instance][criterion]['min_EV']['time to find'],2) for instance in env.instances]\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "df.to_excel('criterions.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darwinian phi rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = 'Darwinian phi rate'\n",
    "# Configurations = {'Darwinian phi rate':{'penalization':['regular','cuadratic','cubic'],\n",
    "#                                         'conservation proportion':[0.2, 0.4, 0.7],\n",
    "#                                         'length restriction':[True, False],\n",
    "#                                         }}\n",
    "\n",
    "Configurations = {'Darwinian phi rate':{'penalization':['cuadratic','cubic'],\n",
    "                                        'conservation proportion':[0.4, 0.7],\n",
    "                                        'length restriction':[True, False],\n",
    "                                        }}\n",
    "\n",
    "keys = list(Configurations[operator].keys())\n",
    "combinations = list(itertools.product(*[Configurations[operator][key] for key in keys]))\n",
    "Grid = [{operator: {keys[i]: combination[i] for i in range(len(keys))}} for combination in combinations]\n",
    "\n",
    "data_Darwin = dict()\n",
    "\n",
    "for instance in env.instances:\n",
    "    data_Darwin[instance] = dict()\n",
    "    for ii in Grid:\n",
    "        config = ii[operator]\n",
    "        identifier = str()\n",
    "        for a in Configurations[operator].keys():\n",
    "            identifier += str(config[a])+str('_')\n",
    "        identifier = identifier[:-1]\n",
    "        data_Darwin[instance][identifier] = plot.retrieve_op_performance(instance, path+f'Experimentation/Operators/{operator}/results-{identifier}-{instance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = env.sizes['l'][9]\n",
    "plot.plot_op_performance(data_Darwin[instance],instance, objective = 'min_EV')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluated insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = 'evaluated insertion'\n",
    "Configurations = {'evaluated insertion':{'penalization':['regular','cuadratic','cubic'],\n",
    "                                         'criterion':['random']}}\n",
    "                                        #  'criterion':['Hybrid', 'phi rate', 'visited costumers']}}#, 'random']}}\n",
    "\n",
    "keys = list(Configurations[operator].keys())\n",
    "combinations = list(itertools.product(*[Configurations[operator][key] for key in keys]))\n",
    "Grid = [{operator: {keys[i]: combination[i] for i in range(len(keys))}} for combination in combinations]\n",
    "\n",
    "data_insert = dict()\n",
    "\n",
    "for instance in env.instances:\n",
    "    data_insert[instance] = dict()\n",
    "    for ii in Grid:\n",
    "        config = ii[operator]\n",
    "        identifier = str()\n",
    "        for a in Configurations[operator].keys():\n",
    "            identifier += str(config[a])+str('_')\n",
    "        identifier = identifier[:-1]\n",
    "        data_insert[instance][identifier] = plot.retrieve_op_performance(path+f'Experimentation/Operators/{operator}/results-{identifier}-{instance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = env.sizes['l'][0]\n",
    "plot.plot_op_performance(data_insert[instance],instance, objective = 'min_EV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Configurations = {'Darwinian phi rate':{'penalization':['cuadratic','cubic'],\n",
    "                                        'conservation proportion':[0.4],\n",
    "                                        'length restriction':[True, False],\n",
    "                                        },\n",
    "                  \n",
    "                  'evaluated insertion':{'penalization':['regular','cuadratic','cubic'],\n",
    "                                         'criterion':['random']},\n",
    "\n",
    "                  'genetic parameters':{'population size':[1500,3000],\n",
    "                                        'crossover rate':[0.3, 0.6],\n",
    "                                        'mutation rate':[0.3, 0.6]}\n",
    "                }\n",
    "\n",
    "D_keys = list(Configurations['Darwinian phi rate'].keys()); D_combinations = list(itertools.product(*[Configurations['Darwinian phi rate'][key] for key in D_keys]))\n",
    "e_keys = list(Configurations['evaluated insertion'].keys()); e_combinations = list(itertools.product(*[Configurations['evaluated insertion'][key] for key in e_keys]))\n",
    "g_keys = list(Configurations['genetic parameters'].keys()); g_combinations = list(itertools.product(*[Configurations['genetic parameters'][key] for key in g_keys]))\n",
    "\n",
    "Grid = [{'Darwinian phi rate': {D_keys[i]: D_combination[i] for i in range(len(D_keys))}, \n",
    "         'evaluated insertion': {e_keys[i]: e_combination[i] for i in range(len(e_keys))}, \n",
    "         'genetic parameters': {g_keys[i]: g_combination[i] for i in range(len(g_keys))}} for D_combination in D_combinations for e_combination in e_combinations for g_combination in g_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploading experimentation!!!\n",
    "\n",
    "test_instances = list()\n",
    "for ii in [5,10,15]:\n",
    "    for i in env.instances:\n",
    "        env.load_data(i)\n",
    "        if len(env.Costumers) == ii:\n",
    "            test_instances.append(i)\n",
    "\n",
    "experiments = range(20)\n",
    "\n",
    "\n",
    "data_exp = dict()\n",
    "\n",
    "for instance in test_instances:\n",
    "    data_exp[instance] = dict()\n",
    "    for num in experiments:\n",
    "        data_exp[instance][num] = plot.retrieve_op_performance(path+f'Experimentation/Exp {num}/results-{instance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Export data to Excel\n",
    "list_idx = 2\n",
    "\n",
    "d = {'instance': [], 'M':[]}#, 'const/#EV':[], 'const/FO':[], 'const/t':[]}\n",
    "for instance in test_instances:\n",
    "    env.load_data(instance)\n",
    "    d['instance'].append(instance)\n",
    "    d['M'].append(len(env.Costumers))\n",
    "\n",
    "for num in experiments:\n",
    "    d[f'{num}/# EV'] = [len(data_exp[instance][num][list_idx]['best individual']) for instance in test_instances]\n",
    "    d[f'{num}/FO'] = [round(data_exp[instance][num][list_idx]['best distance'],2) for instance in test_instances]\n",
    "    # d[f'{criterion}/gap'] = [data[instance][criterion]['gap'] for instance in env.instances]\n",
    "    d[f'{num}/t'] = [round(data_exp[instance][num][list_idx]['time to find'],2) for instance in test_instances]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "df.to_excel('oppp-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Export data to Excel\n",
    "list_idx = 2\n",
    "\n",
    "d = {'instance': [], 'M':[]}#, 'const/#EV':[], 'const/FO':[], 'const/t':[]}\n",
    "for instance in env.instances:\n",
    "    env.load_data(instance)\n",
    "    d['instance'].append(instance)\n",
    "    d['M'].append(len(env.Costumers))\n",
    "\n",
    "for ii in Grid:\n",
    "    config = ii[operator]\n",
    "    identifier = str()\n",
    "    for a in Configurations[operator].keys():\n",
    "        identifier += str(config[a])+str('_')\n",
    "    identifier = identifier[:-1]\n",
    "    d[f'{identifier}/# EV'] = [len(data_insert[instance][identifier][list_idx]['best individual']) for instance in env.instances]\n",
    "    d[f'{identifier}/FO'] = [round(data_insert[instance][identifier][list_idx]['best distance'],2) for instance in env.instances]\n",
    "    # d[f'{criterion}/gap'] = [data[instance][criterion]['gap'] for instance in env.instances]\n",
    "    d[f'{identifier}/t'] = [round(data_insert[instance][identifier][list_idx]['time to find'],2) for instance in env.instances]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "df.to_excel('oppp-results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
